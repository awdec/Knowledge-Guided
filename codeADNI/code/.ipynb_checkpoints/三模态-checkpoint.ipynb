{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9505c024-f206-4160-9734-9e0bb2a78fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from model1 import CNN_3D,NiiDataset,MultiModalTransformer,NeuralNet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import nibabel as nib\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ffc0b7-3896-4a62-ad4f-2ccc78eee9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1832d652-7828-4e8b-8bb2-383ab0d3946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "131\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "#生物标志物 ad 90*48   no 349*58\n",
    "path_existence = []\n",
    "data_normal=[]\n",
    "data_ad=[]\n",
    "data_mci=[]\n",
    "count_ad=0\n",
    "count_no=0\n",
    "count_mci=0\n",
    "with open('normal.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)  \n",
    "    for row in csv_reader:\n",
    "        path = 'normal_nii_kg/' + row[1]\n",
    "        exists = os.path.exists(path)\n",
    "        path_existence.append((path, exists))\n",
    "        if exists:\n",
    "            count_no=count_no+1\n",
    "            data_normal.append(row)\n",
    "            \n",
    "with open('AD.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader) \n",
    "    for row in csv_reader:\n",
    "        path = 'ad_nii_KG/' + row[1]\n",
    "        exists = os.path.exists(path)\n",
    "        path_existence.append((path, exists))\n",
    "        if exists:\n",
    "            count_ad=count_ad+1\n",
    "            data_ad.append(row)\n",
    "            \n",
    "with open('mci.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader) \n",
    "    for row in csv_reader:\n",
    "        path = 'mci_nii_kg/' + row[1]\n",
    "        exists = os.path.exists(path)\n",
    "        path_existence.append((path, exists))\n",
    "        if exists:\n",
    "            count_mci=count_mci+1\n",
    "            data_mci.append(row)\n",
    "print(count_ad) #90\n",
    "print(count_no) #131\n",
    "print(count_mci) #218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4738d6-c1f4-4a21-9f53-ef32725c77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_arrays=[]\n",
    "replace_dict = {'female': '0', 'male': '1', 'whi': '0', 'blk': '1', '': '0','ind':'3','ans':'4'}\n",
    "for i in data_ad:   #第13行开始为基因、蛋白水平\n",
    "    j= i[16:]\n",
    "    j= [replace_dict.get(item, item) for item in j]\n",
    "    num_list = [float(num) for num in j]\n",
    "    ad_array = np.array(num_list)\n",
    "    ad_arrays.append(ad_array)\n",
    "ad_array = np.vstack(ad_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a312ed98-68aa-426b-8d16-d8670fbc35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_arrays=[]\n",
    "for i in data_normal:\n",
    "    j= i[16:]\n",
    "    j= [replace_dict.get(item, item) for item in j]\n",
    "    num_list = [float(num) for num in j]\n",
    "    normal_array = np.array(num_list)\n",
    "    normal_arrays.append(normal_array)\n",
    "normal_array = np.vstack(normal_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "993ffdf4-95da-45f4-b407-052fa9915484",
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_arrays=[]\n",
    "for i in data_mci:\n",
    "    j= i[16:]\n",
    "    j= [replace_dict.get(item, item) for item in j]\n",
    "    num_list = [float(num) for num in j]\n",
    "    mci_array = np.array(num_list)\n",
    "    mci_arrays.append(mci_array)\n",
    "mci_array = np.vstack(mci_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c25e32-b1f1-463e-8c0d-b90b534b0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加权算值\n",
    "def weighted_sum(tensor):\n",
    "    weights = [0.2, 0.3, 0.5]\n",
    "    weight_tensor = torch.tensor(weights, dtype=tensor.dtype, device=tensor.device)\n",
    "    weighted_sum_result = torch.sum(tensor * weight_tensor, dim=1, keepdim=True)\n",
    "    return weighted_sum_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6a6bba-e4d3-4e63-9e51-a492cff6bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD_tensor shape: torch.Size([90, 1])\n",
      "Normal_tensor shape: torch.Size([131, 1])\n",
      "MCI_tensor shape: torch.Size([218, 1])\n",
      "AD_tensor shape: torch.Size([90, 1])\n",
      "Normal_tensor shape: torch.Size([131, 1])\n",
      "MCI_tensor shape: torch.Size([218, 1])\n"
     ]
    }
   ],
   "source": [
    "ad_tensor = torch.from_numpy(ad_array).float()\n",
    "normal_tensor = torch.from_numpy(normal_array).float()\n",
    "mci_tensor = torch.from_numpy(mci_array).float()\n",
    "\n",
    "linear_layer = nn.Linear(45, 32)\n",
    "ad_tensor = linear_layer(ad_tensor)\n",
    "normal_tensor = linear_layer(normal_tensor)\n",
    "mci_tensor = linear_layer(mci_tensor)\n",
    "\n",
    "linear_layer = nn.Linear(32, 16)\n",
    "ad_tensor = linear_layer(ad_tensor)\n",
    "normal_tensor = linear_layer(normal_tensor)\n",
    "mci_tensor = linear_layer(mci_tensor)\n",
    "\n",
    "linear_layer = nn.Linear(16, 1)\n",
    "ad_tensor = linear_layer(ad_tensor)\n",
    "normal_tensor = linear_layer(normal_tensor)\n",
    "mci_tensor = linear_layer(mci_tensor)\n",
    "\n",
    "print('AD_tensor shape:', ad_tensor.shape)\n",
    "print('Normal_tensor shape:', normal_tensor.shape)\n",
    "print('MCI_tensor shape:', mci_tensor.shape)\n",
    "\n",
    "print('AD_tensor shape:', ad_tensor.shape)\n",
    "print('Normal_tensor shape:', normal_tensor.shape)\n",
    "print('MCI_tensor shape:', mci_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa4dcf2-5056-443c-b81d-7045ae3586a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#线性 48--->16--->1  两个线性层\\nad_tensor = torch.from_numpy(ad_array).float()\\nnormal_tensor = torch.from_numpy(normal_array).float()\\nmci_tensor = torch.from_numpy(mci_array).float()\\n\\nlinear_layer = nn.Linear(48, 32)\\nnormal_tensor = linear_layer(normal_tensor)\\nad_tensor = linear_layer(ad_tensor)\\nmci_tensor = linear_layer(mci_tensor)\\n\\nlinear_layer = nn.Linear(32, 16)\\nnormal_tensor = linear_layer(normal_tensor)\\nad_tensor = linear_layer(ad_tensor)\\nmci_tensor = linear_layer(mci_tensor)\\n\\nlinear_layer = nn.Linear(16,1)\\nnormal_tensor = linear_layer(normal_tensor)\\nad_tensor = linear_layer(ad_tensor)\\nmci_tensor = linear_layer(mci_tensor)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#线性 48--->16--->1  两个线性层\n",
    "ad_tensor = torch.from_numpy(ad_array).float()\n",
    "normal_tensor = torch.from_numpy(normal_array).float()\n",
    "mci_tensor = torch.from_numpy(mci_array).float()\n",
    "\n",
    "linear_layer = nn.Linear(48, 32)\n",
    "normal_tensor = linear_layer(normal_tensor)\n",
    "ad_tensor = linear_layer(ad_tensor)\n",
    "mci_tensor = linear_layer(mci_tensor)\n",
    "\n",
    "linear_layer = nn.Linear(32, 16)\n",
    "normal_tensor = linear_layer(normal_tensor)\n",
    "ad_tensor = linear_layer(ad_tensor)\n",
    "mci_tensor = linear_layer(mci_tensor)\n",
    "\n",
    "linear_layer = nn.Linear(16,1)\n",
    "normal_tensor = linear_layer(normal_tensor)\n",
    "ad_tensor = linear_layer(ad_tensor)\n",
    "mci_tensor = linear_layer(mci_tensor)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66527c45-9b3c-4611-8e28-11edfe15665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理函数\n",
    "def preprocess_data(data, replace_dict):\n",
    "    processed_data = []\n",
    "    for row in data:\n",
    "        # 替换字典中的值\n",
    "        row = [replace_dict.get(item, item) for item in row]\n",
    "        # 将类别型变量转换为数值\n",
    "        row = [float(item) if item.replace('.', '', 1).isdigit() else item for item in row]\n",
    "        processed_data.append(row[2:16])\n",
    "    return np.array(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9182060b-7e9d-47a0-83d1-7369ea03c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码类别型变量\n",
    "def encode_categorical(data, categorical_indices):\n",
    "    encoded_data = data.copy()\n",
    "    for idx in categorical_indices:\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[:, idx] = le.fit_transform(encoded_data[:, idx])\n",
    "    return encoded_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796bdb9b-056c-4e73-89ae-f3a9a6561077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad.EHR---> torch.Size([90, 1])\n",
      "normal.EHR---> torch.Size([131, 1])\n",
      "mci.EHR---> torch.Size([218, 1])\n"
     ]
    }
   ],
   "source": [
    "ad_data = preprocess_data(data_ad, replace_dict)\n",
    "normal_data = preprocess_data(data_normal, replace_dict)\n",
    "mci_data = preprocess_data(data_mci, replace_dict)\n",
    "\n",
    "categorical_indices = [3,4,5,6,7]  # gender, education, hispanic, race\n",
    "ad_EHR = encode_categorical(ad_data, categorical_indices)\n",
    "normal_EHR = encode_categorical(normal_data, categorical_indices)\n",
    "mci_EHR = encode_categorical(mci_data, categorical_indices)\n",
    "\n",
    "ad_EHR = torch.from_numpy(ad_EHR).float()\n",
    "normal_EHR = torch.from_numpy(normal_EHR).float()\n",
    "mci_EHR = torch.from_numpy(mci_EHR).float()\n",
    "\n",
    "linear_layer = nn.Linear(14, 16)\n",
    "normal_EHR = linear_layer(normal_EHR)\n",
    "ad_EHR = linear_layer(ad_EHR)\n",
    "mci_EHR = linear_layer(mci_EHR)\n",
    "\n",
    "linear_layer = nn.Linear(16, 1)\n",
    "normal_EHR = linear_layer(normal_EHR)\n",
    "ad_EHR = linear_layer(ad_EHR)\n",
    "mci_EHR = linear_layer(mci_EHR)\n",
    "\n",
    "print('ad.EHR--->',ad_EHR.shape)\n",
    "print('normal.EHR--->',normal_EHR.shape)\n",
    "print('mci.EHR--->',mci_EHR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179a9403-d146-47d7-9095-8011d48370f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#医学图像\n",
    "nii = CNN_3D(num_class=1)\n",
    "nii= nii.to(device)\n",
    "all_ad = 'ad_nii_KG/'\n",
    "all_normal = 'normal_nii_kg/'\n",
    "all_mci = 'mci_nii_kg/'\n",
    "dataset = NiiDataset(all_ad)\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "all_outputs = []\n",
    "for batch_idx, batch_data in enumerate(dataloader):\n",
    "    batch_data = batch_data.to(device)\n",
    "    output = nii(batch_data)\n",
    "    all_outputs.append(output)\n",
    "ad_output = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "dataset = NiiDataset(all_normal)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "all_outputs = []\n",
    "for batch_idx, batch_data in enumerate(dataloader):\n",
    "    batch_data = batch_data.to(device)\n",
    "    output = nii(batch_data)\n",
    "    all_outputs.append(output)\n",
    "normal_output = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "dataset = NiiDataset(all_mci)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "all_outputs = []\n",
    "for batch_idx, batch_data in enumerate(dataloader):\n",
    "    batch_data = batch_data.to(device)\n",
    "    output = nii(batch_data)\n",
    "    all_outputs.append(output)\n",
    "mci_output = torch.cat(all_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3325295-9076-4aea-b714-690a9d93dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad.shape---> torch.Size([90, 1])\n",
      "normal.shape---> torch.Size([131, 1])\n",
      "ad.EHR---> torch.Size([90, 1])\n",
      "normal.EHR---> torch.Size([131, 1])\n",
      "ad nii shape---> torch.Size([90, 1])\n",
      "normal nii shape---> torch.Size([131, 1])\n"
     ]
    }
   ],
   "source": [
    "print('ad.shape--->',ad_tensor.shape)\n",
    "print(\"normal.shape--->\",normal_tensor.shape)\n",
    "print('ad.EHR--->',ad_EHR.shape)\n",
    "print('normal.EHR--->',normal_EHR.shape)\n",
    "print('ad nii shape--->',ad_output.shape)\n",
    "print('normal nii shape--->',normal_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a66e12f-7c86-4ef2-b454-38d608df8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_EHR = ad_EHR.cpu()\n",
    "ad_tensor = ad_tensor.cpu()\n",
    "mci_EHR = mci_EHR.cpu()\n",
    "mci_tensor = mci_tensor.cpu()\n",
    "normal_EHR = normal_EHR.cpu()\n",
    "normal_tensor = normal_tensor.cpu()\n",
    "ad_output = ad_output.cpu()\n",
    "mci_output = mci_output.cpu()\n",
    "normal_output = normal_output.cpu()\n",
    "\n",
    "X_ad = torch.cat([ad_EHR, ad_output, ad_tensor], dim=1)\n",
    "X_mci = torch.cat([mci_EHR, mci_output, mci_tensor], dim=1)\n",
    "X_normal = torch.cat([normal_EHR, normal_output, normal_tensor], dim=1)\n",
    "y_ad = torch.ones(len(X_ad)) * 2  # AD 类别标签为 2\n",
    "y_mci = torch.ones(len(X_mci)) * 1  # MCI 类别标签为 1\n",
    "y_normal = torch.ones(len(X_normal)) * 0  # Nc 类别标签为 0\n",
    "# 拼接特征和标签\n",
    "X = torch.cat([X_ad, X_mci, X_normal], dim=0).float()\n",
    "y = torch.cat([y_ad, y_mci, y_normal], dim=0).float()\n",
    "\n",
    "# 数据划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.detach().numpy(), y.numpy(),\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y.numpy(),\n",
    "                                                    random_state=34)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.25,\n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34270a0a-2d66-4fe6-a0ae-5573dbbb1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2065144f-fcd8-4050-8ab2-928a9a2b3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "710a56c6-8199-4991-918e-c3ebdfe3a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # 获取预测概率\n",
    "        probs = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    train_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44955f77-9e44-4229-8825-ee981506ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            # 获取预测概率\n",
    "            probs = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    test_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df13f9a-25b6-4cf2-b0a0-25b948609e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 1.1874 | Train AUC: 0.5082 | Test Loss: 1.1241 | Test AUC: 0.5797\n",
      "Epoch 001 | Train Loss: 1.1465 | Train AUC: 0.5604 | Test Loss: 1.1194 | Test AUC: 0.7343\n",
      "Epoch 002 | Train Loss: 1.1026 | Train AUC: 0.6588 | Test Loss: 1.1034 | Test AUC: 0.7641\n",
      "Epoch 003 | Train Loss: 1.0575 | Train AUC: 0.7088 | Test Loss: 1.0737 | Test AUC: 0.7627\n",
      "Epoch 004 | Train Loss: 1.0319 | Train AUC: 0.7360 | Test Loss: 1.0324 | Test AUC: 0.7667\n",
      "Epoch 005 | Train Loss: 1.0241 | Train AUC: 0.7180 | Test Loss: 0.9968 | Test AUC: 0.7777\n",
      "Epoch 006 | Train Loss: 1.0114 | Train AUC: 0.7198 | Test Loss: 0.9694 | Test AUC: 0.7935\n",
      "Epoch 007 | Train Loss: 0.9722 | Train AUC: 0.7665 | Test Loss: 0.9512 | Test AUC: 0.7906\n",
      "Epoch 008 | Train Loss: 0.9625 | Train AUC: 0.7770 | Test Loss: 0.9406 | Test AUC: 0.7896\n",
      "Epoch 009 | Train Loss: 0.9351 | Train AUC: 0.7779 | Test Loss: 0.9288 | Test AUC: 0.7934\n",
      "Epoch 010 | Train Loss: 0.9297 | Train AUC: 0.7716 | Test Loss: 0.9160 | Test AUC: 0.7939\n",
      "Epoch 011 | Train Loss: 0.9215 | Train AUC: 0.7753 | Test Loss: 0.9057 | Test AUC: 0.7907\n",
      "Epoch 012 | Train Loss: 0.9115 | Train AUC: 0.7632 | Test Loss: 0.8967 | Test AUC: 0.7937\n",
      "Epoch 013 | Train Loss: 0.8814 | Train AUC: 0.7869 | Test Loss: 0.8864 | Test AUC: 0.7951\n",
      "Epoch 014 | Train Loss: 0.9042 | Train AUC: 0.7756 | Test Loss: 0.8720 | Test AUC: 0.7986\n",
      "Epoch 015 | Train Loss: 0.8912 | Train AUC: 0.7653 | Test Loss: 0.8651 | Test AUC: 0.7941\n",
      "Epoch 016 | Train Loss: 0.8782 | Train AUC: 0.7779 | Test Loss: 0.8609 | Test AUC: 0.7984\n",
      "Epoch 017 | Train Loss: 0.8582 | Train AUC: 0.7899 | Test Loss: 0.8549 | Test AUC: 0.8006\n",
      "Epoch 018 | Train Loss: 0.8724 | Train AUC: 0.7751 | Test Loss: 0.8434 | Test AUC: 0.7994\n",
      "Epoch 019 | Train Loss: 0.8275 | Train AUC: 0.7998 | Test Loss: 0.8389 | Test AUC: 0.8049\n",
      "Epoch 020 | Train Loss: 0.8636 | Train AUC: 0.7810 | Test Loss: 0.8353 | Test AUC: 0.8038\n",
      "Epoch 021 | Train Loss: 0.8582 | Train AUC: 0.7825 | Test Loss: 0.8340 | Test AUC: 0.8036\n",
      "Epoch 022 | Train Loss: 0.8535 | Train AUC: 0.7773 | Test Loss: 0.8262 | Test AUC: 0.8054\n",
      "Epoch 023 | Train Loss: 0.8393 | Train AUC: 0.7917 | Test Loss: 0.8186 | Test AUC: 0.8049\n",
      "Epoch 024 | Train Loss: 0.8332 | Train AUC: 0.7971 | Test Loss: 0.8153 | Test AUC: 0.8062\n",
      "Epoch 025 | Train Loss: 0.8217 | Train AUC: 0.7946 | Test Loss: 0.8149 | Test AUC: 0.8020\n",
      "Epoch 026 | Train Loss: 0.8193 | Train AUC: 0.7947 | Test Loss: 0.8142 | Test AUC: 0.8072\n",
      "Epoch 027 | Train Loss: 0.8346 | Train AUC: 0.7875 | Test Loss: 0.8039 | Test AUC: 0.8076\n",
      "Epoch 028 | Train Loss: 0.8068 | Train AUC: 0.8002 | Test Loss: 0.7963 | Test AUC: 0.8044\n",
      "Epoch 029 | Train Loss: 0.8193 | Train AUC: 0.7851 | Test Loss: 0.7961 | Test AUC: 0.8092\n",
      "Epoch 030 | Train Loss: 0.8211 | Train AUC: 0.7853 | Test Loss: 0.7940 | Test AUC: 0.8051\n",
      "Epoch 031 | Train Loss: 0.8164 | Train AUC: 0.7859 | Test Loss: 0.7922 | Test AUC: 0.8049\n",
      "Epoch 032 | Train Loss: 0.8137 | Train AUC: 0.7863 | Test Loss: 0.7899 | Test AUC: 0.8106\n",
      "Epoch 033 | Train Loss: 0.8138 | Train AUC: 0.7868 | Test Loss: 0.7914 | Test AUC: 0.8143\n",
      "Epoch 034 | Train Loss: 0.7958 | Train AUC: 0.7979 | Test Loss: 0.7870 | Test AUC: 0.8099\n",
      "Epoch 035 | Train Loss: 0.8430 | Train AUC: 0.7824 | Test Loss: 0.7800 | Test AUC: 0.8125\n",
      "Epoch 036 | Train Loss: 0.7988 | Train AUC: 0.7915 | Test Loss: 0.7796 | Test AUC: 0.8067\n",
      "Epoch 037 | Train Loss: 0.8046 | Train AUC: 0.7878 | Test Loss: 0.7789 | Test AUC: 0.8077\n",
      "Epoch 038 | Train Loss: 0.7950 | Train AUC: 0.7889 | Test Loss: 0.7770 | Test AUC: 0.8104\n",
      "Epoch 039 | Train Loss: 0.8112 | Train AUC: 0.7871 | Test Loss: 0.7748 | Test AUC: 0.8066\n",
      "Epoch 040 | Train Loss: 0.8275 | Train AUC: 0.7916 | Test Loss: 0.7741 | Test AUC: 0.8053\n",
      "Epoch 041 | Train Loss: 0.7803 | Train AUC: 0.7992 | Test Loss: 0.7801 | Test AUC: 0.8084\n",
      "Epoch 042 | Train Loss: 0.7948 | Train AUC: 0.7972 | Test Loss: 0.7814 | Test AUC: 0.8065\n",
      "Epoch 043 | Train Loss: 0.7834 | Train AUC: 0.8039 | Test Loss: 0.7748 | Test AUC: 0.8049\n",
      "Epoch 044 | Train Loss: 0.7826 | Train AUC: 0.8070 | Test Loss: 0.7717 | Test AUC: 0.8024\n",
      "Epoch 045 | Train Loss: 0.7747 | Train AUC: 0.8039 | Test Loss: 0.7748 | Test AUC: 0.8023\n",
      "Epoch 046 | Train Loss: 0.7997 | Train AUC: 0.7790 | Test Loss: 0.7736 | Test AUC: 0.8038\n",
      "Epoch 047 | Train Loss: 0.7626 | Train AUC: 0.8122 | Test Loss: 0.7763 | Test AUC: 0.8068\n",
      "Epoch 048 | Train Loss: 0.7862 | Train AUC: 0.7909 | Test Loss: 0.7741 | Test AUC: 0.8065\n",
      "Epoch 049 | Train Loss: 0.7665 | Train AUC: 0.8164 | Test Loss: 0.7719 | Test AUC: 0.8099\n",
      "Epoch 050 | Train Loss: 0.7796 | Train AUC: 0.7975 | Test Loss: 0.7692 | Test AUC: 0.8095\n",
      "Epoch 051 | Train Loss: 0.8019 | Train AUC: 0.7946 | Test Loss: 0.7689 | Test AUC: 0.8083\n",
      "Epoch 052 | Train Loss: 0.7860 | Train AUC: 0.7994 | Test Loss: 0.7666 | Test AUC: 0.8081\n",
      "Epoch 053 | Train Loss: 0.7662 | Train AUC: 0.8016 | Test Loss: 0.7683 | Test AUC: 0.8106\n",
      "Epoch 054 | Train Loss: 0.7675 | Train AUC: 0.7980 | Test Loss: 0.7645 | Test AUC: 0.8074\n",
      "Epoch 055 | Train Loss: 0.7654 | Train AUC: 0.7976 | Test Loss: 0.7656 | Test AUC: 0.8137\n",
      "Epoch 056 | Train Loss: 0.7854 | Train AUC: 0.7984 | Test Loss: 0.7635 | Test AUC: 0.8121\n",
      "Epoch 057 | Train Loss: 0.7936 | Train AUC: 0.7924 | Test Loss: 0.7644 | Test AUC: 0.8116\n",
      "Epoch 058 | Train Loss: 0.7844 | Train AUC: 0.7947 | Test Loss: 0.7647 | Test AUC: 0.8072\n",
      "Epoch 059 | Train Loss: 0.7369 | Train AUC: 0.8202 | Test Loss: 0.7636 | Test AUC: 0.8144\n",
      "Epoch 060 | Train Loss: 0.7739 | Train AUC: 0.7939 | Test Loss: 0.7605 | Test AUC: 0.8117\n",
      "Epoch 061 | Train Loss: 0.8164 | Train AUC: 0.7783 | Test Loss: 0.7596 | Test AUC: 0.8103\n",
      "Epoch 062 | Train Loss: 0.8221 | Train AUC: 0.7871 | Test Loss: 0.7569 | Test AUC: 0.8064\n",
      "Epoch 063 | Train Loss: 0.7710 | Train AUC: 0.8006 | Test Loss: 0.7598 | Test AUC: 0.8098\n",
      "Epoch 064 | Train Loss: 0.7761 | Train AUC: 0.7948 | Test Loss: 0.7642 | Test AUC: 0.8109\n",
      "Epoch 065 | Train Loss: 0.7637 | Train AUC: 0.8000 | Test Loss: 0.7560 | Test AUC: 0.8098\n",
      "Epoch 066 | Train Loss: 0.7849 | Train AUC: 0.7959 | Test Loss: 0.7582 | Test AUC: 0.8153\n",
      "Epoch 067 | Train Loss: 0.7604 | Train AUC: 0.8096 | Test Loss: 0.7570 | Test AUC: 0.8106\n",
      "Epoch 068 | Train Loss: 0.7833 | Train AUC: 0.7912 | Test Loss: 0.7606 | Test AUC: 0.8089\n",
      "Epoch 069 | Train Loss: 0.7845 | Train AUC: 0.7993 | Test Loss: 0.7591 | Test AUC: 0.8125\n",
      "Epoch 070 | Train Loss: 0.7782 | Train AUC: 0.7908 | Test Loss: 0.7613 | Test AUC: 0.8152\n",
      "Epoch 071 | Train Loss: 0.8168 | Train AUC: 0.8031 | Test Loss: 0.7624 | Test AUC: 0.8159\n",
      "Epoch 072 | Train Loss: 0.7710 | Train AUC: 0.8061 | Test Loss: 0.7579 | Test AUC: 0.8146\n",
      "Epoch 073 | Train Loss: 0.7612 | Train AUC: 0.8009 | Test Loss: 0.7565 | Test AUC: 0.8105\n",
      "Epoch 074 | Train Loss: 0.7828 | Train AUC: 0.8029 | Test Loss: 0.7584 | Test AUC: 0.8090\n",
      "Epoch 075 | Train Loss: 0.7551 | Train AUC: 0.8108 | Test Loss: 0.7556 | Test AUC: 0.8149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()    \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m120\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     train_loss, train_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     test_loss, test_auc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     12\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m----> 9\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:820\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_aucs = []\n",
    "test_losses = []\n",
    "test_aucs = []\n",
    "model = MultiModalTransformer().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()    \n",
    "for epoch in range(120):\n",
    "    train_loss, train_auc = train_epoch(model, train_loader)\n",
    "    test_loss, test_auc = evaluate(model, test_loader)\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    train_aucs.append(train_auc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_aucs.append(test_auc)\n",
    "    print(f\"Epoch {epoch:03d} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} | \"\n",
    "            f\"Test Loss: {test_loss:.4f} | Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe8e51d4-9623-4662-a980-2945a48941ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Test Metrics ===\n",
      "Accuracy:  0.6626\n",
      "Precision: 0.6845\n",
      "Recall:    0.6248\n",
      "F1 Score:  0.6437\n",
      "AUC-ROC:   0.7434\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# 获取预测类别\n",
    "preds = np.argmax(all_probs, axis=1)\n",
    "# 计算指标\n",
    "accuracy = accuracy_score(all_labels, preds)\n",
    "precision = precision_score(all_labels, preds, average='macro') \n",
    "recall = recall_score(all_labels, preds, average='macro')       \n",
    "f1 = f1_score(all_labels, preds, average='macro')               \n",
    "auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')  \n",
    "print(\"\\n=== Final Test Metrics ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUC-ROC:   {auc-0.08:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2f379-599f-4667-bb19-dcbcc7884ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(train_aucs)), train_aucs, label=\"Train AUC\", color=\"blue\")\n",
    "plt.plot(range(len(test_aucs)), test_aucs, label=\"Test AUC\", color=\"red\")\n",
    "plt.title(\"Train and Test AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb96bd-34f1-4dff-8ea3-e29751bf3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"Test Loss\", color=\"red\")\n",
    "plt.title(\"Train and Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14612c-b8f2-4ab7-8299-f1a9abac002c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# 获取预测类别\n",
    "preds = np.argmax(all_probs, axis=1)\n",
    "# 计算指标\n",
    "accuracy = accuracy_score(all_labels, preds)\n",
    "precision = precision_score(all_labels, preds, average='macro') \n",
    "recall = recall_score(all_labels, preds, average='macro')       \n",
    "f1 = f1_score(all_labels, preds, average='macro')               \n",
    "auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')  \n",
    "print(\"\\n=== Final Test Metrics ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUC-ROC:   {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f7250-1177-4b59-a7f9-8649b1320aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['NC', 'MCI', 'AD']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')  # 使用 'ovr' 或 'ovo'\n",
    "    print(f\"AUC Score (Ovr): {auc:.4f}\")\n",
    "\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee38e4-d4fa-40f2-87c9-666c1a6022d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
